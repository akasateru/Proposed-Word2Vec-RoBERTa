{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Word2Vec-RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification, logging\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim.downloader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import datasets\n",
    "import string\n",
    "import evaluate\n",
    "import csv \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "logging.set_verbosity_error()\n",
    "logging.set_verbosity_warning()\n",
    "HF_HUB_DISABLE_SYMLINKS_WARNING = True\n",
    "\n",
    "import datetime\n",
    "t_delta = datetime.timedelta(hours=9)\n",
    "JST = datetime.timezone(t_delta, 'JST')\n",
    "now = datetime.datetime.now(JST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "MODEL = \"roberta-base\"\n",
    "SAVED_MODEL = \"../model/Proposed-Word2Vec-RoBERTa_\"+str(now.strftime('%Y%m%d%H%M%S'))\n",
    "THRESHOLD = 0.05\n",
    "MAXLEN_GET_PSEUDO = 3000\n",
    "EPOCH = 1\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/Proposed-Word2Vec-RoBERTa_20221125163758\n"
     ]
    }
   ],
   "source": [
    "print(SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "def preprocessing(text):\n",
    "    # 括弧内文章の削除\n",
    "    text = re.sub(r'\\(.*\\)',' ',text)\n",
    "    text = re.sub(r'\\[.*\\]',' ',text)\n",
    "    text = re.sub(r'\\<.*\\>',' ',text)\n",
    "    text = re.sub(r'\\{.*\\}',' ',text)\n",
    "    # 記号文字の削除\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    # スペースの調整\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 19640.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# 20 newsgroups datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset=\"all\")\n",
    "newsgroups_datasets = list()\n",
    "\n",
    "# example ------------------------------------------------\n",
    "for texts in tqdm(newsgroups.data[:1000]):\n",
    "  texts = texts.split(\"\\n\\n\")\n",
    "  texts = \" \".join(texts[1:])\n",
    "  newsgroups_datasets.append(preprocessing(texts))\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# for texts in tqdm(newsgroups.data):\n",
    "#   texts = texts.split(\"\\n\\n\")\n",
    "#   texts = \" \".join(texts[1:])\n",
    "#   newsgroups_datasets.append(preprocessing(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 49930.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# yahoo topic datasets\n",
    "with open('../data/topic/train_pu_half_v0.txt','r',encoding='utf-8') as f:\n",
    "    texts_v0 = f.read()\n",
    "with open('../data/topic/train_pu_half_v1.txt','r',encoding='utf-8') as f:\n",
    "    texts_v1 = f.read()\n",
    "texts = texts_v0 + texts_v1\n",
    "topic_datasets = list()\n",
    "\n",
    "# example ----------------------------------------------\n",
    "for label_text in tqdm(texts.splitlines()[:1000]):\n",
    "  _, text = label_text.split(\"\\t\")\n",
    "  topic_datasets.append(preprocessing(text))\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# for label_text in tqdm(texts.splitlines()):\n",
    "#   _, text = label_text.split(\"\\t\")\n",
    "#   topic_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 21749.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# reuters datasets\n",
    "with open(\"../data/reuter/sourceall.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  reuter = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "# example -----------------------------------\n",
    "reuter = reuter[:1000]\n",
    "# -------------------------------------------\n",
    "\n",
    "reuters_datasets = list()\n",
    "for label_text in tqdm(reuter):\n",
    "  _, text = label_text.split(\"\\t\")\n",
    "  reuters_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 55436.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# dbpedia datasets train\n",
    "with open('../data/dbpedia_csv/train.csv','r',encoding='utf-8') as f:\n",
    "    reader = [r for r in csv.reader(f)]\n",
    "    \n",
    "# example -------------------\n",
    "reader = reader[:1000]\n",
    "#----------------------------\n",
    "\n",
    "dbpedia_train_datasets = list()\n",
    "for _, auth, text in tqdm(reader):\n",
    "    text = text.replace(auth,'')\n",
    "    dbpedia_train_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbpedia classes\n",
    "with open(\"../data/dbpedia_csv/classes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  classes = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_texts = newsgroups_datasets + topic_datasets + reuters_datasets + dbpedia_train_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "def w2v_avg_vector(sentence):\n",
    "  vector = np.zeros((300,), dtype=\"float32\")\n",
    "  count = 0\n",
    "  for word in sentence.split():\n",
    "    try:\n",
    "      vector = np.add(vector, word2vec[word])\n",
    "      count += 1\n",
    "    except:\n",
    "      pass\n",
    "  if count > 0:\n",
    "    vector = np.divide(vector, len(word))\n",
    "  return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_vector = list()\n",
    "for cls in classes:\n",
    "  classes_vector.append(w2v_avg_vector(cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:01<00:00, 3139.02it/s]\n"
     ]
    }
   ],
   "source": [
    "diff_datasets = {i:[] for i in range(len(classes))}\n",
    "for texts in tqdm(datasets_texts):\n",
    "  texts_vector = w2v_avg_vector(texts)\n",
    "  similarity = cosine_similarity([texts_vector], classes_vector)[0]\n",
    "  sim_argsorted = np.argsort(similarity)\n",
    "  diff = similarity[sim_argsorted[-1]] - similarity[sim_argsorted[-2]]\n",
    "  if diff > THRESHOLD:\n",
    "    diff_datasets[sim_argsorted[-1]].append((similarity[sim_argsorted[-1]], texts))\n",
    "\n",
    "pseudo_texts = list()\n",
    "pseudo_labels = list()\n",
    "for i in range(len(classes)):\n",
    "  sorted_diff_data = sorted(diff_datasets[i], reverse=True)[:MAXLEN_GET_PSEUDO]\n",
    "  pseudo_texts.extend([i[1] for i in sorted_diff_data])\n",
    "  pseudo_labels.extend([i]*len(sorted_diff_data[:MAXLEN_GET_PSEUDO]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all selected data\n",
      "Com. : 273\n",
      "Edu. : 34\n",
      "Art. : 1\n",
      "Ath. : 2\n",
      "Off. : 174\n",
      "Mea. : 210\n",
      "Bui. : 7\n",
      "Nat. : 53\n",
      "Vil. : 1\n",
      "Ani. : 11\n",
      "Pla. : 6\n",
      "Alb. : 24\n",
      "Fil. : 30\n",
      "Wri. : 18\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of all selected data\")\n",
    "for i in diff_datasets:\n",
    "  print(classes[i][:3]+\". : \"+str(len(diff_datasets[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 55476.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "# dbpedia datasets train\n",
    "with open('../data/dbpedia_csv/test.csv','r',encoding='utf-8') as f:\n",
    "    reader = [r for r in csv.reader(f)]\n",
    "    \n",
    "# example -------------------\n",
    "import random\n",
    "reader = random.sample(reader, 1000)\n",
    "#----------------------------\n",
    "\n",
    "test_texts = list()\n",
    "test_labels = list()\n",
    "for labels, auth, text in tqdm(reader):\n",
    "    text = text.replace(auth,'')\n",
    "    test_texts.append(preprocessing(text))\n",
    "    test_labels.append(int(labels)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f36b53646e48c699a2b8545d8e6fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb492a34dcbe434b838fb6d205bfa5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 844\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "train_dataset = datasets.Dataset.from_dict({\"text\":pseudo_texts, \"label\":pseudo_labels})\n",
    "test_dataset = datasets.Dataset.from_dict({\"text\":test_texts, \"label\":test_labels})\n",
    "dataset = datasets.DatasetDict({\"train\":train_dataset, \"test\":test_dataset})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, return_tensors=\"pt\", padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns('text')\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42) #.select(range(5000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42) #.select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return evaluate.load(\"accuracy\").compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=SAVED_MODEL,\n",
    "  num_train_epochs=EPOCH,\n",
    "  per_device_train_batch_size=BATCH_SIZE,\n",
    "  per_device_eval_batch_size=BATCH_SIZE,\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  save_strategy=\"no\",\n",
    "  optim=\"adamw_torch\",\n",
    "  report_to=\"none\"\n",
    "  )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 844\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc74d28fece04d109fcf5c885e62e084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bc2fee5cbe421196738c6d49fb1d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.80893874168396, 'eval_accuracy': 0.167, 'eval_runtime': 38.872, 'eval_samples_per_second': 25.725, 'eval_steps_per_second': 3.216, 'epoch': 1.0}\n",
      "{'train_runtime': 128.2966, 'train_samples_per_second': 6.579, 'train_steps_per_second': 0.826, 'train_loss': 1.2593645779591687, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=106, training_loss=1.2593645779591687, metrics={'train_runtime': 128.2966, 'train_samples_per_second': 6.579, 'train_steps_per_second': 0.826, 'train_loss': 1.2593645779591687, 'epoch': 1.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../model/Proposed-Word2Vec-RoBERTa_20221125163758\\config.json\n",
      "Model weights saved in ../model/Proposed-Word2Vec-RoBERTa_20221125163758\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../model/Proposed-Word2Vec-RoBERTa_20221125163758\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../model/Proposed-Word2Vec-RoBERTa_20221125163758\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../model/Proposed-Word2Vec-RoBERTa_20221125163758\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../model/Proposed-Word2Vec-RoBERTa_20221125163758.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(SAVED_MODEL)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=SAVED_MODEL,report_to=\"none\")\n",
    "trainer = Trainer(model=model, args=training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4520ae6a2da34a509c8f2c41c31d1830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = trainer.predict(small_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Com.      0.132     0.644     0.220        73\n",
      "        Edu.      0.000     0.000     0.000        63\n",
      "        Art.      0.000     0.000     0.000        60\n",
      "        Ath.      0.000     0.000     0.000        94\n",
      "        Off.      0.636     0.092     0.161        76\n",
      "        Mea.      0.050     0.086     0.063        58\n",
      "        Bui.      0.000     0.000     0.000        68\n",
      "        Nat.      0.079     0.338     0.129        74\n",
      "        Vil.      0.000     0.000     0.000        74\n",
      "        Ani.      0.000     0.000     0.000        57\n",
      "        Pla.      0.000     0.000     0.000        71\n",
      "        Alb.      0.403     0.988     0.572        84\n",
      "        Fil.      0.000     0.000     0.000        70\n",
      "        Wri.      0.000     0.000     0.000        78\n",
      "\n",
      "    accuracy                          0.167      1000\n",
      "   macro avg      0.093     0.153     0.082      1000\n",
      "weighted avg      0.101     0.167     0.090      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\akasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\akasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = [np.argmax(i) for i in pred.predictions]\n",
    "\n",
    "target_names = [c[:3]+\".\" for c in classes]\n",
    "\n",
    "rep = classification_report(pred.label_ids, y_pred, target_names=target_names, digits=3)\n",
    "print(rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e32349538976b425bbf8209bec3a52ef38eb988b8b568d4d0fb100f86dbbec2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
